{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Combining Neural Networks with Gradient Boosting"
      ],
      "metadata": {
        "id": "BL8bXcTAX6Tr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Transforming Data\n",
        "We will be working with the CIFAR 10 dataset. A popular dataset for benchmarking image classification performance."
      ],
      "metadata": {
        "id": "8qM-Ho6kYF1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Bb5GYq6HavYf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),               # Resize to 256x256 pixels\n",
        "    transforms.CenterCrop(224),           # Crop the image to 224x224 pixels about the center\n",
        "    transforms.ToTensor(),                # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize with ImageNet mean\n",
        "                         std=[0.229, 0.224, 0.225])   # Normalize with ImageNet std\n",
        "])\n",
        "# Load the training and test sets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# Splitting the training set into training and validation sets\n",
        "train_size = int(0.8 * len(trainset))  # 80% of the dataset for training\n",
        "val_size = len(trainset) - train_size  # 20% of the dataset for validation\n",
        "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "trainloader = DataLoader(train_dataset, batch_size=4,\n",
        "                         shuffle=True, num_workers=2)\n",
        "valloader = DataLoader(val_dataset, batch_size=4,\n",
        "                       shuffle=False, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=4,\n",
        "                        shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR-10:\n",
        "# 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QNNNEJnYEsN",
        "outputId": "df60243d-8c12-4fd3-c306-fba513632d0b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load pre-trained ResNet-50\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "\n",
        "# Remove the last fully connected layer (classification layer)\n",
        "# to use the model as a feature extractor\n",
        "modules = list(resnet50.children())[:-1]\n",
        "resnet50 = nn.Sequential(*modules)\n",
        "\n",
        "for p in resnet50.parameters():\n",
        "    p.requires_grad = False  # Freeze the parameters\n",
        "\n",
        "resnet50.eval()  # Set the model to evaluation mode\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQnND4z1Y-C5",
        "outputId": "75309c33-406b-41b8-f047-c474114bf5e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 159MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def extract_features(dataloader):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            # Move inputs to the device the model is on\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            features = resnet50.to(DEVICE)(inputs)\n",
        "\n",
        "            # Flatten the features to a 1D vector\n",
        "            features = features.view(features.size(0), -1)\n",
        "\n",
        "            features_list.append(features.cpu().numpy())\n",
        "            labels_list.append(labels.cpu().numpy())\n",
        "\n",
        "    # Concatenate all features and labels into a single NumPy array\n",
        "    features_array = np.concatenate(features_list, axis=0)\n",
        "    labels_array = np.concatenate(labels_list, axis=0)\n",
        "\n",
        "    return features_array, labels_array\n",
        "\n",
        "# Extract features for train, validation, and test sets\n",
        "train_features, train_labels = extract_features(trainloader)\n",
        "val_features, val_labels = extract_features(valloader)\n",
        "test_features, test_labels = extract_features(testloader)\n"
      ],
      "metadata": {
        "id": "jmWc72KvZRPd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create DMatrix for XGBoost\n",
        "dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
        "dval = xgb.DMatrix(val_features, label=val_labels)\n",
        "dtest = xgb.DMatrix(test_features, label=test_labels)\n",
        "\n",
        "# XGBoost parameters\n",
        "params = {\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': 10\n",
        "}\n",
        "\n",
        "# Train XGBoost model\n",
        "bst = xgb.train(params, dtrain, num_boost_round=100, evals=[(dval, 'eval')], early_stopping_rounds=10)\n",
        "\n",
        "# Save the model\n",
        "bst.save_model('xgb_model.json')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4inbEJbaT-L",
        "outputId": "7aeff6d5-b245-447f-aee2-69541a01c20a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\teval-mlogloss:2.08032\n",
            "[1]\teval-mlogloss:1.91896\n",
            "[2]\teval-mlogloss:1.78940\n",
            "[3]\teval-mlogloss:1.68011\n",
            "[4]\teval-mlogloss:1.58597\n",
            "[5]\teval-mlogloss:1.50446\n",
            "[6]\teval-mlogloss:1.43190\n",
            "[7]\teval-mlogloss:1.36728\n",
            "[8]\teval-mlogloss:1.30894\n",
            "[9]\teval-mlogloss:1.25662\n",
            "[10]\teval-mlogloss:1.20796\n",
            "[11]\teval-mlogloss:1.16333\n",
            "[12]\teval-mlogloss:1.12275\n",
            "[13]\teval-mlogloss:1.08542\n",
            "[14]\teval-mlogloss:1.05093\n",
            "[15]\teval-mlogloss:1.01879\n",
            "[16]\teval-mlogloss:0.98890\n",
            "[17]\teval-mlogloss:0.96061\n",
            "[18]\teval-mlogloss:0.93534\n",
            "[19]\teval-mlogloss:0.91109\n",
            "[20]\teval-mlogloss:0.88886\n",
            "[21]\teval-mlogloss:0.86780\n",
            "[22]\teval-mlogloss:0.84811\n",
            "[23]\teval-mlogloss:0.82900\n",
            "[24]\teval-mlogloss:0.81147\n",
            "[25]\teval-mlogloss:0.79498\n",
            "[26]\teval-mlogloss:0.77901\n",
            "[27]\teval-mlogloss:0.76443\n",
            "[28]\teval-mlogloss:0.75073\n",
            "[29]\teval-mlogloss:0.73726\n",
            "[30]\teval-mlogloss:0.72444\n",
            "[31]\teval-mlogloss:0.71260\n",
            "[32]\teval-mlogloss:0.70121\n",
            "[33]\teval-mlogloss:0.69047\n",
            "[34]\teval-mlogloss:0.67994\n",
            "[35]\teval-mlogloss:0.67045\n",
            "[36]\teval-mlogloss:0.66120\n",
            "[37]\teval-mlogloss:0.65264\n",
            "[38]\teval-mlogloss:0.64396\n",
            "[39]\teval-mlogloss:0.63567\n",
            "[40]\teval-mlogloss:0.62806\n",
            "[41]\teval-mlogloss:0.62049\n",
            "[42]\teval-mlogloss:0.61343\n",
            "[43]\teval-mlogloss:0.60645\n",
            "[44]\teval-mlogloss:0.60009\n",
            "[45]\teval-mlogloss:0.59364\n",
            "[46]\teval-mlogloss:0.58748\n",
            "[47]\teval-mlogloss:0.58172\n",
            "[48]\teval-mlogloss:0.57615\n",
            "[49]\teval-mlogloss:0.57069\n",
            "[50]\teval-mlogloss:0.56563\n",
            "[51]\teval-mlogloss:0.56093\n",
            "[52]\teval-mlogloss:0.55590\n",
            "[53]\teval-mlogloss:0.55139\n",
            "[54]\teval-mlogloss:0.54676\n",
            "[55]\teval-mlogloss:0.54239\n",
            "[56]\teval-mlogloss:0.53810\n",
            "[57]\teval-mlogloss:0.53421\n",
            "[58]\teval-mlogloss:0.53061\n",
            "[59]\teval-mlogloss:0.52685\n",
            "[60]\teval-mlogloss:0.52313\n",
            "[61]\teval-mlogloss:0.51959\n",
            "[62]\teval-mlogloss:0.51613\n",
            "[63]\teval-mlogloss:0.51285\n",
            "[64]\teval-mlogloss:0.50944\n",
            "[65]\teval-mlogloss:0.50651\n",
            "[66]\teval-mlogloss:0.50345\n",
            "[67]\teval-mlogloss:0.50064\n",
            "[68]\teval-mlogloss:0.49788\n",
            "[69]\teval-mlogloss:0.49502\n",
            "[70]\teval-mlogloss:0.49228\n",
            "[71]\teval-mlogloss:0.48952\n",
            "[72]\teval-mlogloss:0.48693\n",
            "[73]\teval-mlogloss:0.48473\n",
            "[74]\teval-mlogloss:0.48225\n",
            "[75]\teval-mlogloss:0.47984\n",
            "[76]\teval-mlogloss:0.47735\n",
            "[77]\teval-mlogloss:0.47499\n",
            "[78]\teval-mlogloss:0.47297\n",
            "[79]\teval-mlogloss:0.47090\n",
            "[80]\teval-mlogloss:0.46883\n",
            "[81]\teval-mlogloss:0.46668\n",
            "[82]\teval-mlogloss:0.46492\n",
            "[83]\teval-mlogloss:0.46296\n",
            "[84]\teval-mlogloss:0.46115\n",
            "[85]\teval-mlogloss:0.45931\n",
            "[86]\teval-mlogloss:0.45754\n",
            "[87]\teval-mlogloss:0.45569\n",
            "[88]\teval-mlogloss:0.45408\n",
            "[89]\teval-mlogloss:0.45255\n",
            "[90]\teval-mlogloss:0.45112\n",
            "[91]\teval-mlogloss:0.44964\n",
            "[92]\teval-mlogloss:0.44820\n",
            "[93]\teval-mlogloss:0.44668\n",
            "[94]\teval-mlogloss:0.44515\n",
            "[95]\teval-mlogloss:0.44362\n",
            "[96]\teval-mlogloss:0.44202\n",
            "[97]\teval-mlogloss:0.44066\n",
            "[98]\teval-mlogloss:0.43949\n",
            "[99]\teval-mlogloss:0.43806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on the test set\n",
        "preds = bst.predict(dtest)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.sum(preds == test_labels) / len(test_labels)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My7KwvKJaZ7W",
        "outputId": "021fc162-7d95-463a-af40-8edff1738cfc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.41%\n"
          ]
        }
      ]
    }
  ]
}
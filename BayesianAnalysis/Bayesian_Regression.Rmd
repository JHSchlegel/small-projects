---
title: "Bayesian Data Analysis"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rstanarm)
library(rstan)
library(tidyverse)
```
stan_glm uses Hamiltonian MCMC
```{r}
data(women)

gaussian_glm_mcmc <- stan_glm(formula = height ~ weight, data = women,
                              algorithm = "sampling", refresh = 0)
summary(gaussian_glm_mcmc)

gaussian_glm_mle <- glm(formula = height ~ weight, data = women)
summary(gaussian_glm_mle)
```

Bayesian: parameter random, interval fixed
Frequentist: parameter fixed, interval random


```{r}
posterior_interval(gaussian_glm_mcmc, prob = .95)
confint(gaussian_glm_mle)
```


```{r}
prior_summary(gaussian_glm_mcmc)
```

prior vs adjusted priors:

unadjusted:
$$
a_k\sim \mathcal{N}(0, 2.5 ), \quad \text{where}\quad
y = b + a_1x_1 + a_2x_2+\cdots+a_nx_n, \\


\sigma \sim \mathcal{E}xp(1)
$$
adjusted:
$$
a_k\sim \mathcal{N}\left(0, 2.5 \frac{sd(y)}{sd(x)}\right), \quad \text{where}\quad
y = b + a_1x_1 + a_2x_2+\cdots+a_nx_n, \\


\sigma \sim \mathcal{E}xp\left(\frac{1}{sd(y)}\right)
$$
Intercept has to be centered

$$
b_{intercept}\sim\mathcal{N}(\bar{y},  2.5\cdot sd(y))
$$



```{r}
gaussian_glm2 <- stan_glm(formula = height ~ weight, data = women,
                              algorithm = "sampling", 
                              prior = normal(4, 1),
                              prior_intercept = normal(1, 15),
                              prior_aux =  exponential(rate = 2),
                          cores = 32, chains = 32)
summary(gaussian_glm2)
prior_summary(gaussian_glm2)
posterior_vs_prior(gaussian_glm2, pars = "beta")
posterior_vs_prior(gaussian_glm2, pars = "alpha")
```

Flat priors -> slower convergence as it takes a lot longer to discover parameter space
```{r}
glm_uninformative_prior <- stan_glm(formula = height ~ weight, data = women,
                              algorithm = "sampling", 
                              prior = NULL,
                              prior_intercept = NULL,
                              prior_aux =  exponential(rate = 2),
                          cores = 4, chains = 4)
prior_summary(glm_uninformative_prior)
posterior_vs_prior(glm_uninformative_prior, pars = "beta")
posterior_vs_prior(glm_uninformative_prior, pars = "alpha")
```

```{r}
?plot.stanreg
plot(gaussian_glm2, "hist")
```
```{r}
as.data.frame(gaussian_glm2) %>% 
  rowid_to_column("iter") %>% 
  pivot_longer(-iter, names_to = "parameter", values_to = "value") %>% 
  ggplot(aes(x = value)) + 
  geom_histogram(color = "white", fill = "skyblue") +
  scale_fill_brewer(palette = "Set2") +
  theme_bw() +
  facet_wrap(~parameter, scale = "free")
```

```{r}
library(bayesplot)
library(ggplot2)

y <- women$height
women_posterior <- posterior_predict(gaussian_glm2, draws = 1000) # 1000 predictions for each woman's height

# Graphic Posterior Predictive Distribution:
color_scheme_set("brightblue")
ppc_dens_overlay(y = y, yrep = women_posterior[1:50, ])
```


```{r}
quadratic_glm <- stan_glm(formula = height ~ weight + I(weight^2), data = women, cores = 32,
         chains = 32)

print(quadratic_glm)
summary(quadratic_glm)
```
Rhat is 1, thus model has converged




```{r}
quadratic_women_posterior <- posterior_predict(quadratic_glm, draws = 1000)


ppc_dens_overlay(y, quadratic_women_posterior[1:50,]) # better fit than linear model
```

```{r}
cubic_glm <- stan_glm(formula = height ~ weight + I(weight^2) + I(weight^3), data = women, cores = 32,
         chains = 32)
cubic_women_posterior <- posterior_predict(cubic_glm, draws = 1000)


ppc_dens_overlay(y, cubic_women_posterior[1:50,]) # worse fit than quadratic model
```


```{r}
ppc_hist(y, quadratic_women_posterior[1:30,], binwidth = 2)
ppc_boxplot(y, quadratic_women_posterior[1:30,], binwidth = 2)
ppc_boxplot(y, women_posterior[1:30,], binwidth = 2)

pp_check(quadratic_glm, "boxplot", nreps = 10) # already first 15 samples very good
```


```{r}
# MCMC plots
mcmc_intervals(quadratic_glm, pars = "weight", prob = .95)
mcmc_areas_ridges(quadratic_glm, pars = "weight", prob = .95)
```




```{r}
data(state.x77)

glm_multi <- stan_glm(data = as.data.frame(state.x77), formula = Murder ~., cores = 32, chains = 32, refresh = 0)
mcmc_intervals(glm_multi, prob = .95)
mcmc_trace(glm_multi)
mcmc_acf(glm_multi, pars = "Population")
glm_multi %>%   
  rhat() %>%
  mcmc_rhat() +
  yaxis_text()
# all values are below 1.05 suggesting no convergence issues

glm_multi %>% 
  neff_ratio() %>% 
  mcmc_neff() +
  yaxis_text()
# ratio of effective sample size to total sample size -> lighter/higher is better


library(coda)
As.mcmc.list(glm_multi$stanfit) %>% gelman.plot()

```


```{r}
library(effects)
data(mtcars)

# with no interactions
no_interaction <- glm(mpg ~ hp + wt, data = mtcars)
summary(no_interaction)
# with interactions
interaction <- glm(mpg ~ hp + wt + hp:wt, data = mtcars)
summary(interaction)


model.matrix(~ hp + wt + hp:wt, data = mtcars)

plot(effect("hp:wt", interaction), multiline = TRUE)
```

